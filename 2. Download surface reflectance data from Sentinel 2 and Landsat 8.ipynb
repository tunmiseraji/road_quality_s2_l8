{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31fa759",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70000353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:51:07.488185Z",
     "start_time": "2024-09-28T13:51:07.476482Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import geopandas as gpd\n",
    "import ee\n",
    "# ee.Authenticate()\n",
    "# ee.Initialize()\n",
    "import os\n",
    "import time\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fddb74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:14:11.019478Z",
     "start_time": "2024-09-28T13:14:09.796696Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf_combined_split_clean = gpd.read_feather('data/roads_sub_split_clean.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a4966",
   "metadata": {},
   "source": [
    "# Extract pixel values and save to connected Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19afcde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:38:39.570063Z",
     "start_time": "2024-09-28T13:38:39.556628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract pixel values from an image\n",
    "def extract_values(feature, image, sensor_name, year, band, sl):\n",
    "    image = image.select(band)\n",
    "    sampled = image.sample(region=feature.geometry(), scale=sl)\n",
    "    sampled_values = sampled.aggregate_array(band)\n",
    "    return ee.Feature(None, {\n",
    "        'SampledValues': sampled_values,\n",
    "        'Sensor': sensor_name,\n",
    "        'Status': feature.get('Status'), \n",
    "        'Unique_ID': feature.get('Unique_ID'),\n",
    "        'Unique_ID_Fraction': feature.get('Unique_ID_Fraction'),\n",
    "        'Year': year,\n",
    "        'Band': band\n",
    "    })\n",
    "\n",
    "# Function to scale DN to reflectance for Landsat 8\n",
    "def scale_to_reflectance_l8(img):\n",
    "    optical_bands = img.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "    return img.addBands(optical_bands, None, True)\n",
    "\n",
    "# Function to scale DN to reflectance for Sentinel-2\n",
    "def scale_to_reflectance_s2(img):\n",
    "    return img.multiply(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971466ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:50:51.928288Z",
     "start_time": "2024-09-28T13:50:51.917089Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes a while to download for regional studies. Takes about 3 hours for Rwanda from 2013 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfee9016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:42:43.029956Z",
     "start_time": "2024-09-28T13:41:48.384906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2023\n",
      "All Landsat tasks completed for the current batch.\n"
     ]
    }
   ],
   "source": [
    "new_gdf = gdf_combined_split_clean # gdf_combined_split_clean_rw\n",
    "\n",
    "# Load image collections with cloud filters applied\n",
    "l8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filter(ee.Filter.lt('CLOUD_COVER', 20))\n",
    "s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "\n",
    "# Define start and end years\n",
    "start_year_l8 = 2023 # changed 2013 to 2023 when creating multitemporal road quality maps for Rwanda\n",
    "end_year = 2023\n",
    "start_year_s2 = 2018\n",
    "\n",
    "batch_size = 500  # Adjust this value if export fails because of memory error\n",
    "\n",
    "\n",
    "# Iterate over each year to process data\n",
    "for year in range(start_year_l8, end_year + 1):\n",
    "    print(f\"Processing year: {year}\")\n",
    "    start_date, end_date = f\"{year}-01-01\", f\"{year}-12-31\"\n",
    "    \n",
    "    # Prepare Landsat 8 yearly composite\n",
    "    l8_yearly = l8.filterDate(start_date, end_date).map(scale_to_reflectance_l8).median()\n",
    "    l8_yearly = l8_yearly.addBands(l8_yearly.expression(\n",
    "        \"1 - ((3 * min(min(SR_B6, SR_B5), SR_B2)) / (SR_B6 + SR_B5 + SR_B2))\",\n",
    "        {'SR_B6': l8_yearly.select('SR_B6'), 'SR_B5': l8_yearly.select('SR_B5'), 'SR_B2': l8_yearly.select('SR_B2')}\n",
    "    ).rename('Road_Index')).addBands(l8_yearly.normalizedDifference(['SR_B6', 'SR_B5']).rename('NDBI'))\n",
    "\n",
    "    # Process batches of features\n",
    "    for start in range(0, len(new_gdf), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = new_gdf.iloc[start:end]\n",
    "        \n",
    "        # Create a feature collection for the batch\n",
    "        features = [ee.Feature(ee.Geometry.MultiLineString(row.geometry.__geo_interface__['coordinates']), \n",
    "                               {'Status': row['Status'], 'Unique_ID': row['Unique_ID'], 'Unique_ID_Fraction': row['Unique_ID_Fraction']}) \n",
    "                    for _, row in batch.iterrows()]\n",
    "        fc = ee.FeatureCollection(features)\n",
    "        \n",
    "        # Process each band for Landsat 8\n",
    "        tasks = []\n",
    "        for band in ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']:\n",
    "            l8_extracted = fc.map(lambda feature: extract_values(feature, l8_yearly, 'Landsat', year, band, 30))\n",
    "            description = f'{band}_batch_{start}_year_{year}'\n",
    "            task = ee.batch.Export.table.toDrive(\n",
    "                collection=l8_extracted,\n",
    "                description=description,\n",
    "                folder='landsat_test',\n",
    "                fileNamePrefix=description,\n",
    "                fileFormat='CSV'\n",
    "            )\n",
    "            task.start()\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # Monitor task status\n",
    "        for task in tasks:\n",
    "            while task.status()['state'] in ['READY', 'RUNNING']:\n",
    "                time.sleep(10)\n",
    "        print(\"All Landsat tasks completed for the current batch.\")\n",
    "        \n",
    "        # Prepare and process Sentinel-2 data if applicable\n",
    "        if year >= start_year_s2:\n",
    "            s2_yearly = s2.filterDate(start_date, end_date).map(scale_to_reflectance_s2).median()\n",
    "            s2_yearly = s2_yearly.addBands(s2_yearly.expression(\n",
    "                \"1 - ((3 * min(min(B11, B8), B2)) / (B11 + B8 + B2))\",\n",
    "                {'B11': s2_yearly.select('B11'), 'B8': s2_yearly.select('B8'), 'B2': s2_yearly.select('B2')}\n",
    "            ).rename('Road_Index')).addBands(s2_yearly.normalizedDifference(['B11', 'B8']).rename('NDBI'))\n",
    "\n",
    "            # Process each band for Sentinel-2\n",
    "            for band in ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']:\n",
    "                s2_extracted = fc.map(lambda feature: extract_values(feature, s2_yearly, 'Sentinel', year, band, 10))\n",
    "                description = f'{band}_batch_{start}_year_{year}'\n",
    "                task = ee.batch.Export.table.toDrive(\n",
    "                    collection=s2_extracted,\n",
    "                    description=description,\n",
    "                    folder='sentinel_test',\n",
    "                    fileNamePrefix=description,\n",
    "                    fileFormat='CSV'\n",
    "                )\n",
    "                task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492958ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download csvs and save in a folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a02f1",
   "metadata": {},
   "source": [
    "# Combine CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573776b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the year and obtain unique countries\n",
    "year = '2019' # or 2023\n",
    "unique_countries = gdf_combined_split_clean['NAME_EN'].unique()\n",
    "\n",
    "# Folders\n",
    "folders = {\n",
    "    'Landsat': 'data' + year + '/landsat',\n",
    "    'Sentinel': 'data' + year + '/sentinel'\n",
    "}\n",
    "\n",
    "# Loop through both folders\n",
    "for sensor, folder in folders.items():\n",
    "    # List all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder, file)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop unnecessary columns\n",
    "        df = df.drop(columns=['.geo', 'Year', 'system:index'])\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame for the sensor\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "    # Process each country in the combined DataFrame\n",
    "    for country in unique_countries:\n",
    "        # Get unique IDs for the country\n",
    "        rw_ids = gdf_combined_split_clean[gdf_combined_split_clean.NAME_EN == country].Unique_ID.unique()\n",
    "        \n",
    "        # Filter the DataFrame for the current country's unique IDs\n",
    "        df_filtered = combined_df[combined_df.Unique_ID.isin(rw_ids)]\n",
    "        \n",
    "        # Convert 'SampledValues' column from string to list and explode it\n",
    "        df_filtered['SampledValues'] = df_filtered['SampledValues'].apply(ast.literal_eval)\n",
    "        df_filtered = df_filtered.explode('SampledValues')\n",
    "        \n",
    "        # Create a new unique ID column and a counter for pivoting\n",
    "        df_filtered['Unique_ID_new'] = df_filtered['Unique_ID'].astype(str) + '_' + df_filtered['Unique_ID_Fraction'].astype(str)\n",
    "        df_filtered['Counter'] = df_filtered.groupby(['Unique_ID_new', 'Band']).cumcount()\n",
    "\n",
    "        # Pivot the DataFrame and merge with other relevant columns\n",
    "        pivot_df = df_filtered.pivot(index=['Unique_ID_new', 'Counter'], columns='Band', values='SampledValues').reset_index()\n",
    "        other_columns_df = df_filtered[['Unique_ID_new', 'Sensor', 'Status']].drop_duplicates()\n",
    "        pivot_df = pivot_df.merge(other_columns_df, on='Unique_ID_new', how='left').drop(columns='Counter')\n",
    "\n",
    "        # Save the pivoted DataFrame as a Feather file\n",
    "        output_filename = f'data/pivoted/{country}_{sensor}_{year}.feather'\n",
    "        pivot_df.to_feather(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c4f1ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:19:46.772569Z",
     "start_time": "2024-09-28T14:19:46.759668Z"
    }
   },
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe07e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "gee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
